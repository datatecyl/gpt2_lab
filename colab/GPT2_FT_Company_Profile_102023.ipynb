{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agm8kvVRU2Ue"
      },
      "outputs": [],
      "source": [
        "# Fine tuning gpt2_medium model and use own data like company profile\n",
        "#\n",
        "# See also medium.com blog\n",
        "# \"GPT-2 Fine-Tuning Guide: Building a Chatbot for Your Company Profile\"\n",
        "# https://medium.com/@datatec.studio\n",
        "#\n",
        "\n",
        "# Mount google driver\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to google driver folder which contains datasets\n",
        "# This folder will also be used to save model\n",
        "%cd /content/drive/MyDrive/GPT2_Lab_DTS"
      ],
      "metadata": {
        "id": "B-j40wpGU8y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "MHzJziLsVCcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "from chat_data import ChatData"
      ],
      "metadata": {
        "id": "s5rI-cgkxRYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define environment variable, path of data, model name and device\n",
        "os.environ[\"HF_HOME\"] = \"/content/huggingface\"  # Replace with your desired directory\n",
        "print(\"Please replace it with your hf access token:\")\n",
        "os.environ[\"HF_HOME_TOKEN\"] = \"Please_replace_it_with_your_hf_access_token\"\n",
        "\n",
        "result_dir = '/content/drive/MyDrive/GPT2_Lab_DTS/results'\n",
        "data_file_path = '/content/drive/MyDrive/GPT2_Lab_DTS/data/my_company_info.json'\n",
        "\n",
        "model_name = \"gpt2-medium\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "T6fuYpH4VE17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a python file to google driver\n",
        "# Sample of json datasets\n",
        "# You can also directly upload this code to your google driver\n",
        "# The code write here in this way is for better understanding of whole project\n",
        "%%writefile chat_data.py\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "\n",
        "class ChatData(Dataset):\n",
        "    def __init__(self, path: str, tokenizer):\n",
        "        self.data = json.load(open(path, \"r\"))\n",
        "\n",
        "        self.X = []\n",
        "        for i in self.data:\n",
        "            for j in i['dialog']:\n",
        "                self.X.append(j['text'])\n",
        "\n",
        "        for idx, i in enumerate(self.X):\n",
        "            try:\n",
        "                self.X[idx] = \"<startofstring> \" + i + \" <bot>: \" + self.X[idx + 1] + \" <endofstring>\"\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        for i in self.data:\n",
        "            for j in i['dialog']:\n",
        "                self.X.append(j['text'])\n",
        "\n",
        "        total_samples = len(self.X)  # Calculate the total number of samples\n",
        "        print(\"total_samples\", total_samples)\n",
        "        # define samples amount\n",
        "        self.X = self.X[:500]\n",
        "        print(\"Here is the self.X[0] i wanna check:\")\n",
        "        print(self.X[0])\n",
        "\n",
        "        self.X_encoded = tokenizer(self.X, return_tensors=\"pt\", max_length=30, padding=\"max_length\", truncation=True)\n",
        "        self.input_ids = self.X_encoded['input_ids']\n",
        "        self.attention_mask = self.X_encoded['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attention_mask[idx]\n"
      ],
      "metadata": {
        "id": "_ZgHPQgRuKbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model, save model and tokernize to harddisk\n",
        "## prepare tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                            \"bos_token\": \"<startofstring>\",\n",
        "                            \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])\n",
        "\n",
        "## prepare model\n",
        "### Specify the desired embedding size (must be a multiple of 8)\n",
        "desired_embedding_size = 50264  # Change this to the desired size\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "### Resize the embedding layer to the desired size\n",
        "model.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
        "model = model.to(device)\n",
        "\n",
        "## save tokenizer and model to harddisk\n",
        "tokenizer.save_pretrained(result_dir)\n",
        "model.save_pretrained(result_dir)"
      ],
      "metadata": {
        "id": "_HLw88IBQHml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## load model and tokenizer from harddisk\n",
        "### Load the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(result_dir)\n",
        "\n",
        "### Load the GPT-2 model from the local folder\n",
        "model = GPT2LMHeadModel.from_pretrained(result_dir)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "C9CzY7I9Qjh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define infer and train function\n",
        "def infer(inp):\n",
        "  inp = \"<startofstring> \" + inp + \" <bot>: \"\n",
        "  inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "  X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
        "  a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
        "\n",
        "  output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
        "\n",
        "  output = tokenizer.decode(output[0])\n",
        "\n",
        "  return output\n",
        "\n",
        "def train(chatData, model, optim):\n",
        "\n",
        "  epochs = 12\n",
        "\n",
        "  for _ in tqdm.tqdm(range(epochs)):  # Use range() to iterate through epochs\n",
        "      for X, a in chatData:\n",
        "          print(X)\n",
        "          X = X.to(device)\n",
        "          a = a.to(device)\n",
        "          optim.zero_grad()\n",
        "          loss = model(input_ids=X, attention_mask=a, labels=X).loss\n",
        "          loss.backward()\n",
        "          optim.step()\n",
        "\n",
        "  # Save the model's state dictionary after training is complete\n",
        "  torch.save(model.state_dict(), \"model_state.pt\")\n",
        "  print(infer(\"How do you see the integration of holographic technology in education?\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tuKpppvpVlUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load ChatData, train model and optimizer\n",
        "chatData = ChatData(data_file_path, tokenizer)\n",
        "chatData = DataLoader(chatData, batch_size=64)\n",
        "\n",
        "model.train()\n",
        "\n",
        "optim = Adam(model.parameters())"
      ],
      "metadata": {
        "id": "xQrezTeDcsz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 10 times\n",
        "epochs = 10  # You can adjust the number of epochs as needed\n",
        "for epoch in range(epochs):\n",
        "    print(\"Round: \", epoch)\n",
        "    train(chatData, model, optim)"
      ],
      "metadata": {
        "id": "7VWe95ug3Bum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"\"\n",
        "while True:\n",
        "  inp = input(\"Enter your input (press Enter when done): \" + \" \" * 20)\n",
        "  print(infer(inp))"
      ],
      "metadata": {
        "id": "toCb6YovWMu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-mSe8nV-MUk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}